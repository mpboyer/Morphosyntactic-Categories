\documentclass{cours}
\usepackage{nicematrix}
\title{Sur la Stabilité Interlangue des Catégories Morphosyntaxiques\\[2pt]\small Rapport de Stage de L3}
\author{Matthieu Boyer}
\date{Été 2024}

\begin{document}
\titlepage % Output the title page
	{}
	{\centering %
		{\Huge Sur\, la\, Stabilité\, Interlangue\, des\, Catégories\, Morphosyntaxiques\par}
                \vspace{16pt}
                {\Large Rapport de Stage de L3\par}
                \vspace{24pt}
                {\huge Matthieu \textsc{Boyer}\par}}
	{../logo_lattice}
	{\centering %
                {\huge\sc Laboratoire\, Lattice \par}
                \vspace{16pt}
                {\large \sc CNRS --- ENS-PSL --- Université Sorbonne Nouvelle\par}
                \vspace{24pt}
		{\Large Sous la direction de Mathieu \textsc{Dehouck}\par}}

\tableofcontents

\begin{abstract}
Dans ce rapport, nous nous intéressons à la stabilité interlangue des catégories morphosyntaxiques.
Nous nous sommes interrogés quant à la réalité de la comparaison interlangue de catégories descriptives du langage, et tout particulièrement aux cas grammaticaux.
Nous avons observé les différences dans la forme syntaxique de certains cas, et remarqué les similarités entre d'autres plus sémantiquement distincts, en observant toutefois une forme géométrique générale.
Nous avons déterminé des prototypes syntaxiques de cas et étudié la représentation des adpositions.
\end{abstract}


\section{Introduction}\label{sec:introduction}
\subsection{Contextualisation}\label{subsec:contextualisation}
\begin{quote}
	There is a fundamental distinction between language-particular categories of languages (which descriptive linguists must describe by descriptive categories of their descriptions) and comparative concepts (which comparative linguists may use to compare languages).
	{\flushright \textit{Martin Haspelmath} \cite{Has18}}
\end{quote}

Selon Haspelmath, il est possible que la manière de décrire les langues en linguistique soit basée sur des envies de comparaison, parfois mal placées.
Dans ce rapport, nous allons donc nous intéresser à la notion fondamentale de catégorie morphosyntaxique, et comparer les descriptions dans différents langages de catégories linguistiques comparatives.
Ceci permettrait de justifier la transposition de résultats d'une langue vers une autre.

\medskip
\label{subsec:linguistique}
En linguistique, la morphologie est l'étude des mots, de la manière dont ils sont formés et des relations entre eux au sein du langage.
La syntaxe est l'étude de la manière dont se combinent les morphèmes (plus petites unités de son faisant sens dans un langage) et les mots pour former des structures plus grandes comme des phrases.
La sémantique, enfin, est l'étude du sens linguistique, de comment les mots ont du sens, et de la manière dont le sens de parties d'une phrase influe sur le sens de celle-ci.\\
La morphosyntaxe est la combinaison des aspects morphologiques et syntaxiques du langage, et examine notamment comment les formes des mots et structures grammaticales interagissent pour transmettre du sens dans une phrase.
Une catégorie morphosyntaxique est une propriété syntaxique, c'est à dire ayant des influences sur la structure grammaticale de la phrase, qui est marquée morphologiquement sur certains mots.
En français par exemple, le pluriel est une catégorie morphosyntaxique: il est marqué à la fin des mots et donne des informations sur quel groupe gouverne un autre groupe.
Il existe également des catégories morphosémantiques, qui ne donnent que des informations sur la signification d'une phrase et pas sur sa structure.\\
\medskip
Les cas grammaticaux sont des exemples de catégories morphosyntaxiques et morphosémantiques.

\subsection{Données et \textsc{Universal Dependencies}}\label{subsec:données}
Pour étudier la thèse d'Haspelmath, nous allons considérer que les relations de dépendances (\textit{reldep}) décrites par les annotations de \textsc{Universal Dependencies} (UD, version 2 décrite dans \cite{UDv2}) sont une manière de représenter des catégories comparatives.
Une relation de dépendance est une manière de décrire les relations syntaxiques dans une phrase (cf \ref{fig:ud_annot}).
Elles se déduisent de la construction par une grammaire de dépendance ou contextuelle de la phrase.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{Figures/Visualisations/simplified_ud_annotation}
	\caption{Représentation d'une Phrase en Anglais et en Finnois et de ses Relations de Dépendances, source:\cite{UDv2}}
	\label{fig:ud_annot}
\end{figure}

Il existe 37 relations de dépendances de base, mais les personnes annotant les corpus ont la possibilité d'en créer de nouvelles, sous la syntaxe \texttt{str1:str2:\dots} où \texttt{str1} doit être une relation de dépendance décrite comme relation basique dans la table 3 de \cite{UDv2}.\\
Par ailleurs, les mots sont annotés avec les propriétés morphologiques qu'ils possèdent, par exemple leur temps ou leur aspect pour un verbe ou leur cas et leur genre pour un nom.
Les propriétés morphologiques universelles utilisées par UD sont décrites table 2 dans \cite{UDv2}.\\
Enfin, les mots sont annotés avec leur nature grammaticale (e.g. Nom, Verbe, Pronom\dots) comme décrit table 1 dans \cite{UDv2}.

\subsection{Méthode}\label{subsec:méthode}
Nous considérons tout d'abord que chaque \textit{reldep} décrit une unique catégorie comparative et que plusieurs \textit{reldep} ne peuvent instancier une même catégorie comparative.
En comptant le nombre d'instances de chaque \textit{reldep} pour un mot vérifiant une propriété grammaticale de la langue (i.e.\ une catégorie descriptive, que l'on représente par une propriété morphologique d'UD, typiquement les cas pour des langues en utilisant), on obtient une représentation des catégories descriptives et on peut donc mesurer la proximité de deux catégories descriptives dans deux langues différentes.
Les corpus utilisés dans cette première partie sont ceux du projet \textsc{Universal Dependencies}, version 2.14 décrite dans \cite{UD214}.\\
On n'utilise jamais la signification sémantique a priori d'un cas pour en induire un résultat. C'est pourquoi on ne donne la définition théorique d'aucun cas, sauf pour donner un exemple ou expliquer un résultat.

\section{Approche Géométrique}\label{sec:géométrie}
Pour cette première approche, on considère la représentation obtenue comme une représentation vectorielle: on considère qu'on se place sur $\R^{\abs{R}}$ où $R$ est l'ensemble des \textit{reldep} et dont une base est l'ensemble des relations de dépendance.
Ceci nous permet d'étudier la structure syntaxique de chaque cas d'une manière géométrique.
Dans la suite, on notera $\nu(T, C)$ la représentation du cas $C$ dans le corpus $T$ avec la convention $\nu(T, C) = 0_{\R^{\abs{R}}}$ si $T$ ne contient pas de mot au cas $C$.
On définit également $\mE(C)$ l'ensemble des corpus $T$ pour lesquels $\nu(T, C)$ est non nul et $\cont(T)$ l'ensemble des cas $C$ tels que $\nu(T, C)$ est non nul.

\subsection{Avec la distance Cosinus}\label{subsec:cosinus}
On mesure d'abord la proximité de deux vecteurs en utilisant la distance\footnote{Ce n'est pas une distance au sens mathématique} cosinus entre ceux-ci:
\begin{equation}
	d_{\cos}(v_{1},v_{2}) = \frac{\scalar{v_{1}\mid v_{2}}}{\norm{v_{1}}\norm{v_{2}}}
\end{equation}

Ici, on considère le produit scalaire canonique sur $\R^{\abs{R}}$.
On calcule alors, pour $C_{1}, C_{2}$ deux cas donnés, l'ensemble des $d_{\cos}\left(\nu\left(T_{1}, C_{1}\right), \nu\left(T_{2}, C_{2}\right)\right)$ où $T_{1}, T_{2} \in \mE(C_{1})\times \mE(C_{2})$.
Ceci nous donne des informations statistiques sur la proximité angulaire (donc la proximité des directions) de deux cas.
On obtient alors les résultats décrits dans \ref{tab:angle_gen}, \ref{tab:angle_nom} et \ref{tab:angle_voc}.

\renewcommand{\arraystretch}{1.1}
\begin{table}
        \centering
        \resizebox{.9\textwidth}{!}{\begin{NiceTabular}{ccccccccc}
                Cas & Abl & Acc & Dat & Gen & Ins & Loc & Nom & Voc \\
                Premier Quartile & 0.037 & 0.020 & 0.022 & 0.032 & 0.056 & 0.027 & 0.026 & 0.000 \\
                Médiane & 0.198 & 0.123 & 0.134 & 0.317 & 0.249 & 0.188 & 0.104 & 0.006 \\
                Troisième Quartile & 0.416 & 0.302 & 0.341 & 0.823 & 0.449 & 0.400 & 0.225 & 0.047 \\
                Moyenne & 0.259 & 0.196 & 0.214 & 0.421 & 0.282 & 0.243 & 0.159 & 0.058 \\
        \CodeAfter
                \begin{tikzpicture}
                        \foreach \i in {1,...,6}
                                {\draw[draw=vulm] (1|-\i) -- (10|-\i);}
                        \draw[draw=vulm] (2|-1)--(2|-6);\end{tikzpicture}
        \end{NiceTabular}}
	\captionof{table}{Proximité Angulaire pour le \textit{Génitif}}
	\label{tab:angle_gen}
	\vspace{\abovecaptionskip}

	\renewcommand{\arraystretch}{1.1}
        \centering
        \resizebox{.9\textwidth}{!}{\begin{NiceTabular}{ccccccccc}
                Cas& Abl & Acc & Dat & Gen & Ins & Loc & Nom & Voc \\
                Premier Quartile & 0.020 & 0.038 & 0.018 & 0.026 & 0.035 & 0.020 & 0.620 & 0.003 \\
                Médiane & 0.067 & 0.137 & 0.072 & 0.104 & 0.113 & 0.078 & 0.815 & 0.026 \\
                Troisième Quartile & 0.158 & 0.272 & 0.161 & 0.225 & 0.211 & 0.156 & 0.912 & 0.075 \\
                Moyenne & 0.115 & 0.188 & 0.119 & 0.159 & 0.153 & 0.115 & 0.739 & 0.072 \\
        \CodeAfter
                \begin{tikzpicture}
                        \foreach \i in {1,...,6}
                                {\draw[draw=vulm] (1|-\i) -- (10|-\i);}
                        \draw[draw=vulm] (2|-1)--(2|-6);\end{tikzpicture}
        \end{NiceTabular}}
	\captionof{table}{Proximité Angulaire pour le \textit{Nominatif}}
	\label{tab:angle_nom}
	\vspace{\abovecaptionskip}

	\renewcommand{\arraystretch}{1.1}
        \centering
        \resizebox{.9\textwidth}{!}{\begin{NiceTabular}{ccccccccc}
                Cas & Abl & Acc & Dat & Gen & Ins & Loc & Nom & Voc \\
                Premier Quartile & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.003 & 0.500 \\
                Médiane & 0.004 & 0.007 & 0.006 & 0.006 & 0.012 & 0.007 & 0.026 & 0.885 \\
                Troisième Quartile & 0.025 & 0.040 & 0.032 & 0.047 & 0.055 & 0.038 & 0.075 & 0.973 \\
                Moyenne & 0.035 & 0.042 & 0.036 & 0.058 & 0.053 & 0.042 & 0.072 & 0.681 \\
        \CodeAfter
                \begin{tikzpicture}
                        \foreach \i in {1,...,6}
                                {\draw[draw=vulm] (1|-\i) -- (10|-\i);}
                        \draw[draw=vulm] (2|-1)--(2|-6);\end{tikzpicture}
        \end{NiceTabular}}
	\captionof{table}{Proximité Angulaire pour le \textit{Vocatif}}
	\label{tab:angle_voc}
\end{table}


On observe notamment le fait que le nominatif comme le vocatif ont des directions très particulières, et sont très différents de tous les autres cas. Pour le reste, les résultats sont assez flous, et il semble difficile de tirer des résultats généraux.

\medskip
Néanmoins, cette méthode est très limitée. En effet, on ne considère ici que 9 des 45 cas définis dans au moins un corpus.
De plus, les résultats donnés ici sont à pondérer par la présence de nombreux corpus/langages ne possédant pas au moins l'un des cas ci-dessus, ce qui amène à une représentation trop brouillée des informations.


\subsection{Avec l'algorithme de \textsc{Zassenhaus}}\label{subsec:zassenhaus}
On considère les espaces vectoriels engendrés par la représentation vectorielle du système de cas d'une langue, que l'on appellera \emph{espaces de cas}.
Ceux-ci sont d'une certaine dimension finie.
On applique alors sur toute paire de système de cas l'algorithme de Zassenhaus (voir \cite{zassenhaus}), permettant de générer une base de l'espace somme et de l'espace intersection.\\
Toutefois, la grande variance au niveau des coordonnées, et la trop faible dimension (au plus 45, mais souvent de l'ordre de 5) dans un grand espace (dimension 228), rend l'intersection toujours nulle numériquement.
De plus, cet algorithme est très lent à exécuter car il demande de nombreux appels mémoire pour obtenir la matrice de l'espace de cas de chaque paire de cas, et demande de trouver une base de l'espace de colonnes, ce qui n'est pas nécessairement la famille de colonnes de la matrice.

\subsection{Angle entre Cas et Système de Cas}\label{subsec:projection}
On considère à nouveau la distance cosinus, mais cette fois-ci, non pas entre deux vecteurs, mais entre un vecteur et un espace de cas.
Ceci est fait en considérant le projeté orthogonal d'un vecteur sur un espace de cas et en mesurant l'angle entre les deux (ou la distance cosinus).
En observant les données de plus près, on trouve une valeur inattendue: l'angle entre le vocatif du farsi et le système de cas arabe, deux langues syntaxiquement proches, est de l'ordre de $\arccos{10^{-16}}$.
En regardant de plus près les corpus en farsi, on observe que cela découle d'une idiosyncrasie\footnote{Caractère propre à une langue} dans les annotations.
En farsi, le lemme (unité morphologique abstraite: \textsl{fais} et \textsl{fait} sont deux graphies du même lemme \textsl{faire}, conjugué à deux personnes différentes)\label{farsi}
est décrit comme une interjection portant le vocatif et se reliant à un nom au cas absolu par la relation de transmission de cas (c'est à dire de marquer le cas pour un autre mot).
Ce lemme agit donc en réalité plus comme une adposition\footnote{Mot-outil immédiatement associé à un élément subordonné qui en précise les relations syntaxiques et sémantiques avec le reste de la phrase}.
Le vocatif n'apparaît que très peu en farsi, et majoritairement dans cette situation.\\
Ainsi, il semble que nous ne pouvons pas appliquer de propriétés apprises du farsi à une autre langue, du moins sur le système de cas.
Il est toutefois bon de noter que plusieurs corpus adoptent cette convention d'annotation des adpositions (cf \ref{subsec:adpos}) et certains l'adoptent également sur certains adverbes.\\
Par ailleurs, il n'est pas rare qu'au sein d'une même langue, deux corpus produisent des résultats assez différents. Ceci peut venir de la variance des phrases considérées, mais plus souvent de la présence ou non des reldeps \texttt{conj, case} et de la manière d'annoter le cas d'une adposition, comme décrit plus haut..

\subsection{Distance Euclidienne}\label{subsec:l2}
On considère cette fois la distance euclidienne entre tous deux vecteurs, qu'on aura au préalable normalisés pour qu'ils représentent des distributions de probabilités.
On calcule alors, pour $C_{1}, C_{2}$ deux cas donnés, l'ensemble des $d\left(\nu\left(T_{1}, C_{1}\right), \nu\left(T_{2}, C_{2}\right)\right)$ où $T_{1}, T_{2} \in \mE(C_{1})\times \mE(C_{2})$ et $d$ est la distance euclidienne associée au produit scalaire canonique.
On obtient le tableau \ref{tab_dist_acc}

\renewcommand{\arraystretch}{1.1}
\begin{table}
        \centering
        \resizebox{.9\textwidth}{!}{\begin{NiceTabular}{ccccccccc}
                Cas & Abl & Acc & Dat & Gen & Ins & Loc & Nom & Voc \\
                Premier Quartile & 0.605 & 0.315 & 0.552 & 0.521 & 0.535 & 0.595 & 0.569 & 0.842 \\
                Médiane & 0.781 & 0.476 & 0.725 & 0.715 & 0.695 & 0.764 & 0.722 & 0.992 \\
                Troisième Quartile & 1.002 & 0.695 & 0.950 & 0.936 & 0.929 & 0.993 & 0.965 & 1.115 \\
                Moyenne & 0.802 & 0.526 & 0.751 & 0.733 & 0.732 & 0.790 & 0.760 & 0.982 \\
        \CodeAfter
                \begin{tikzpicture}
                        \foreach \i in {1,...,6}
                                {\draw[draw=vulm] (1|-\i) -- (10|-\i);}
                        \draw[draw=vulm] (2|-1)--(2|-6);\end{tikzpicture}
        \end{NiceTabular}}
        \caption{Proximité pour la distance euclidienne avec l'\textit{Accusatif}}
	\label{tab_dist_acc}
\end{table}

On utilise ces données pour déterminer, entre deux corpus (ici le \texttt{Czech-CLTT} et le \texttt{Russian-GSD}), quel cas de l'autre langage est le plus proche d'un cas du premier.
Dans l'exemple de \ref{tab_reldep_ru_cz}, le datif russe est plus proche du génitif tchèque que du datif tchèque.

\begin{table}
	\centering
	\begin{tabular}{>{\tt}r|cccc}
		\toprule
		&Dat RU & Gen CZ & Gen RU & Dat CZ\\
		\midrule
		Total & 1711 & 2631 & 2070 & 277\\
		\midrule
		obl & 450 & 208 & 219 & 48\\
		iobj & 340 & 0 & 0 & 0\\
		amod & 243 & 736 & 475 & 54\\
		nmod & 300 & 1000 & 980 & 24\\
		conj & 112 & 225 & 84 & 21\\
		case & 0 & 340 & 1 & 87\\
		det & 34 & 80 & 79 & 3\\
		\bottomrule
	\end{tabular}
	\caption{Extraits des vecteurs de \textit{Reldep} pour le Russe et le Tchèque}
	\label{tab_reldep_ru_cz}
\end{table}

On enlève ensuite \texttt{conj, det} puisque ces \textit{reldep} démontrent l'accord vers la tête du groupe/de la proposition, et donc des doublons dans les données, ceci permet d'éviter de compter comme plusieurs instances d'un même cas un groupe nominal de la forme \emph{Alice, le boulanger, la laitière et Bob}, qui remplit un usage sémantique et syntaxique unique dans la phrase.
On enlève aussi \texttt{case}, qui est souvent utilisé (on peut notamment le voir dans l'exemple ci-dessous) pour marquer le cas avec une adposition, et ceci dépend très fortement de la personne qui a annoté le corpus, et de l'usage dans les grammaires du langage.\\
\medskip

On observe les relations de proximités entre les cas de deux langues en considérant le graphe orienté des plus proches voisins.
Celui-ci permet de découvrir quels groupes de cas ont les mêmes fonctions syntaxiques dans les deux langues, et pourraient être interchangés lors de l'analyse grammaticale (parsing), cf \ref{fig_gnn_ru_cz}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{Figures/GNN/gnn_ru_gsd_cs_fictree}
	\caption{Graphes des Plus Proches Voisins Russe-Tchèque.}
	\label{fig_gnn_ru_cz}
\end{figure}

Pour éviter encore plus d'avoir des redites, on décide de ne se concentrer que sur des mots de même nature.
Ceci permet par exemple d'éviter qu'un groupe nominal ayant la même fonction sémantique (e.g. objet direct du verbe) apporte plusieurs instances d'un même cas (e.g. avec un adjectif et un nom à l'accusatif).
On obtient alors le graphe \ref{fig_gnn_nouns_ru_cz} pour le tchèque et le russe en ne considérant que les noms:

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{Figures/GNN/gnn_ru_gsd_cs_fictree_Nouns_Only}
	\caption{Graphes des Plus Proches Voisins Russe-Tchèque pour les Noms uniquement.}
	\label{fig_gnn_nouns_ru_cz}
\end{figure}

On observe notamment que pour les noms, la structure du graphe reste la même.
Le vocatif russe et le vocatif tchèque, peu utilisés et le partitif russe n'ayant pas d'équivalent en tchèque, ils sont bien plus éloignés des autres cas.
On retrouve par ailleurs un bloc datif -- génitif qui était déjà présent auparavant, à variance dans le corpus près.
Par ailleurs, on observe également que les paires accusatif -- accusatif et nominatif -- nominatif restent stables et plus proches que toutes les autres paires de cas.
En ne considérant que les pronoms, on obtient le graphe \ref{fig_gnn_pronouns_ru_cz}.


\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{Figures/GNN/gnn_ru_gsd_cs_fictree_Pronouns_Only}
	\caption{Graphes des Plus Proches Voisins Russe-Tchèque pour les Pronoms uniquement.}
	\label{fig_gnn_pronouns_ru_cz}
\end{figure}

Cette fois ci, il y a une variance bien plus forte dans les distances, sans doute due à la variance dans les données. En effet, à part au nominatif et à l'accusatif les échantillons de données sont bien plus faibles (cf \ref{tab_echantillons}).

\begin{table}
	\centering
	\framebox{\begin{minipage}{.45\textwidth}
	\begin{center}
	\emph{Échantillons pour les noms:}
	\medskip
	\begin{tabular}{ccc}
		\toprule
		Cas & Russe & Tchèque\\
		\midrule
		Acc & 2807 & 5960\\
		\midrule
		Dat & 1029 & 861\\
		\midrule
		Gen & 7616 & 4378\\
		\midrule
		Ins & 1642 & 2100\\
		\midrule
		Loc & 2809 & 2583\\
		\midrule
		Nom & 4571 & 5970\\
		\midrule
		Par & 1 & 0\\
		\midrule
		Voc & 1 & 203\\
		\bottomrule
	\end{tabular}
	\end{center}
	\end{minipage}}
	\framebox{
	\begin{minipage}{.45\textwidth}
	\begin{center}
	\emph{Échantillons pour les pronoms:}
	\medskip
	\begin{tabular}{ccc}
		\toprule
		Cas & Russe & Tchèque\\
		\midrule
		Acc & 206 & 5960\\
		\midrule
		Dat & 129 & 2743\\
		\midrule
		Gen & 241 & 448\\
		\midrule
		Ins & 151 & 400\\
		\midrule
		Loc & 128 & 221\\
		\midrule
		Nom & 631 & 1427\\
		\midrule
		Par & 1 & 0\\
		\midrule
		Voc & 1 & 8\\
		\bottomrule
	\end{tabular}
	\end{center}
	\end{minipage}}
	\caption{Taille d'Échantillons sur les cas en Russe et en Tchèque.}
	\label{tab_echantillons}
\end{table}

Finalement, il semble que considérer les pronoms fait perdre en information car ceux-ci sont bien moins usités en général. Toutefois, on remarque également qu'une structure générale de la langue semble transparaître de ces graphes.
Il faut cependant noter que les graphes dépendent très fortement du corpus proposé comme le montre \ref{GNNRuCz}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{Figures/GNN/gnn_ru_gsd_cs_cltt}
	\caption{Graphe des Plus Proches Voisins Russe-Tchèque}
	\label{GNNRuCz}
\end{figure}

Ici, on ne considère plus le corpus \texttt{cs\_fictree} mais le \texttt{cs\_cltt}, bien plus petit (467 phrases contre 10160).
Même si la forme du graphe ne semble que peu changer, la variance dans le corpus joue énormément.\\
Par ailleurs, cette méthode n'est que peu applicable, puisqu'elle nécessite d'étudier toutes les langues par paire.

\subsection{Visualisation des Données}\label{subsec:vis}
Les résultats précédemment obtenus ne permettent d'observer les données ou bien qu'à une échelle très importante, ce qui brouille les résultats, ou bien qu'à une échelle trop faible pour qu'on puisse généraliser un résultat.
Dans la suite, on propose donc des méthodes pour regrouper les vecteurs de cas dans différentes langues, afin d'essayer de constater l'uniformité (ou non) de certains groupes de cas.
L'objectif est ici de réussir à repérer, au sein des données, des patterns, afin de pouvoir mener les analyses présentées plus haut sur des groupes de corpus intéressants.


\subsubsection{PCA}\label{subsub:pca}
On commence par appliquer une analyse en deux composantes principales (PCA) aux vecteurs représentant deux cas différents.
Cette analyse va procéder à une réduction linéaire de la dimension des données de départ afin de trouver deux vecteurs (les composantes) qui forment un système de coordonnées pour lequel les variations selon chaque axe sont les plus fortes.
Pour ceci, on calcule la première composante comme combinaison linéaire des vecteurs d'entrée qui maximise la variance des projections des vecteurs d'entrée sur cette première composante. Puis on recommence pour la seconde composante en prenant un vecteur orthogonal à la première composante.

\begin{figure}
	\begin{minipage}{.5\textwidth}
	\begin{center}
	\includegraphics[width=\linewidth]{Figures/Visualisations/pca_Gen_Nom}
	\end{center}
	\end{minipage}
	\begin{minipage}{.5\textwidth}
	\begin{center}
	\includegraphics[width=\linewidth]{Figures/Visualisations/pca_Gen_Nom_Nouns}
	\end{center}
	\end{minipage}
	\caption{Représentations de l'Analyse en deux Composantes Principales sur le Génitif et le Nominatif}
	\label{fig:pca}
\end{figure}

En regardant les corpus qui sont éloignés du groupe principal (cf \ref{fig:pca}), on observe que les composantes déterminées par l'algorithme de PCA sont en fait basées sur deux langues, et donc les deux composantes n'ont pas de sens au regard de la syntaxe.

\subsubsection{Analyse Topologique des Données}\label{subsub:tda}
Dans ce paragraphe, on applique des méthodes topologiques dans l'espoir d'obtenir une manière de différencier (ou de regrouper) des ensembles de données (les $\nu(T, C)$ pour $T\in \mE(C)$).
En effet, on essaie de reconstruire topologiquement la variété dont les vecteurs d'entrée forment une triangulation\footnote{i.e. la représentation de la variété par des espaces linéaires par morceaux, des simplexes (voir ci-dessous)}, ce qui permet d'obtenir des invariants algébriques et de les comparer entre deux variétés.

On rappelle qu'un complexe simplicial est une sorte de généralisation à plusieurs dimensions des graphes. Le complexe simplicial $k$-dimensionnel d'un ensemble $X = \{x_{0}, \ldots, x_{k}\}$ de points est l'enveloppe convexe de $X$.
Les points de $X$ sont appelés sommets et les simplexes\footnote{Généralisation à $n$-dimension du triangle. Le tétraèdre est un simplexe en dimension $3$} engendrés par les parties de $X$ sont appelés faces du complexe.
Le $k$-ème groupe d'homologie $H_{k}$ est un espace vectoriel dont la dimension représente le nombre de \og\emph{trous}\fg\ $k$-dimensionnels.
$H_{0}$ représente ainsi les composantes connexes, $H_{1}$ les boucles unidimensionnelles et $H_{2}$ les cavités $2$-dimensionnelles.
Les nombres de Betti sont les dimensions des groupes d'homologie et donnent donc le nombre de \og\emph{trous}\fg\ $k$-dimensionnels (voir \cite{tldrtda} pour plus de détails).

Pour l'implémentation de ce qui suit, on utilise les bibliothèques Python \texttt{Gudhi}, \texttt{POT} et \texttt{Hera}, dont les implémentations sont décrites dans~\cite{Gudhi},\cite{PythonPOT},~\cite{Hera} qui permettent de calculer efficacement les complexes simpliciaux des données et leurs diagrammes d'homologie, la distance de Wasserstein (ou Earth Mover's Distance) entre deux distributions ou diagrammes d'homologie.

On utilise d'abord un complexe cubique plutôt qu'un complexe simplicial pour essayer de représenter plus efficacement les groupes d'homologie de la variété triangulée par les points de chaque cas.
Un complexe cubique est un ensemble $\mathcal{K}$ de cubes\footnote{produits d'intervalles} tel que la frontière de chaque cube soit dans $\mathcal{K}$ (voir \cite{cubicalhomology}).
On utilise un complexe cubique puisqu'ici, il est compliqué de calculer directement un complexe simplicial de par le nombre de points et la dimension de l'espace.

\begin{figure}
\begin{minipage}{.5\textwidth}
	\begin{center}
	\includegraphics[width=\linewidth]{Figures/Visualisations/cc_Abl}
	\end{center}
\end{minipage}
\begin{minipage}{.5\textwidth}
	\begin{center}
	\includegraphics[width=\linewidth]{Figures/Visualisations/cc_Acc}
	\end{center}
\end{minipage}
\caption{Représentations de l'Homologie Persistente du Complexe Cubique sur $\{\tt Abl,Acc\}$}
\label{fig_cc_hom}
\end{figure}

On a également calculé les complexes sur les Nominatifs et Accusatifs. On remarque sur \ref{fig_cc_hom} qu'une forme générale se retrouve dans les diagrammes de persistence de chacun des cas.
Pour vérifier l'intuition, on calcule la distance-$1$ de Wasserstein entre les distributions de points. On rappelle que la distance-$p$ de Wasserstein pour $p \in \left[1, +\infty\right]$ entre deux mesures de probabilités $\mu, \pi$ dont les moments d'ordre $p$ sont finis est:
\begin{equation*}
W_{p}(\mu, \pi) = \inf_{\gamma\in\Gamma\left(\mu, \pi\right)}\sqrt[p]{\mathbb{E}_{(x, y) \sim \gamma}d(x, y)^{p}}
\end{equation*}
où $\Gamma\left(\mu, \pi\right)$ est l'ensemble des couplages\footnote{Loi de probabilité à deux variables dont les lois marginales sont $\mu, \pi$} de $\mu, \pi$.

\begin{table}
\centering
\begin{tabular}{c|ccccc}
	\toprule
	Cas & Abl & Acc & Dat & Loc & Gen\\
	\midrule
	Abl & 0.00 & 2.45 & 3.11 & 1.94 & 3.85\\
	Acc & 2.45 & 0.00 & 1.33 & 1.25 & 1.79\\
	Dat & 3.11 & 1.33 & 0.00 & 1.63 & 1.27\\
	Loc & 1.94 & 1.25 & 1.63 & 0.00 & 2.26\\
	Gen & 3.85 & 1.79 & 1.27 & 2.26 & 0.00\\
	\bottomrule
\end{tabular}
\caption{Distances de Wasserstein entre les Diagrammes de Persistence des Complexes Cubiques pour quelques Cas}
\label{tab_wass_cc}
\end{table}

Les distances présentées dans \ref{tab_wass_cc} étant assez faibles compte tenu le nombre de points (on n'a, contrairement à \ref{subsec:bary}, pas des distributions de probabilité), on obtient bien le résultat suggéré par les figures, il semble y avoir une structure générale de la notion topologique de variété engendrée par un cas.
Pour vérifier cette hypothèse, on teste de même en créant le complexe de Rips\footnote{Généralisation des graphes à $\alpha$-voisinage, c'est l'ensemble des simplexes dont tous les sommets sont à distance au plus une constante}, avec une valeur de paramètre assez faible $\alpha = .5$.
Toutefois, les diagrammes présentés dans \ref{fig_rc_hom} restant très brouillons, il est difficile de trouver une variété plus abstraite avec la même homologie.

\begin{figure}
\begin{minipage}{.5\textwidth}
	\begin{center}
	\includegraphics[width=\linewidth]{Figures/Visualisations/rc_Abl.pdf}
	\end{center}
\end{minipage}
\begin{minipage}{.5\textwidth}
	\begin{center}
	\includegraphics[width=\linewidth]{Figures/Visualisations/rc_Acc.pdf}
	\end{center}
\end{minipage}
\caption{Représentations de l'Homologie Persistente du Complexe de Rips sur $\{\tt Abl,Acc\}$}
\label{fig_rc_hom}
\end{figure}

On remarque à nouveau qu'une forme générale se retrouve dans les diagrammes de persistence de chacun des cas.
Lorsqu'on calcule la distance de Wasserstein entre deux diagrammes pour quelques cas, on obtient le tableau suivant:

\begin{table}
\centering
\begin{tabular}{c|cccccc}
	\toprule
	Cas & Abl & Acc & Dat & Gen & Loc & Nom\\
	\midrule
	Abl & 0.00 & 0.89 & 1.27 & 0.82 & 1.09 & 0.94\\
	Acc & 0.89 & 0.00 & 1.01 & 0.42 & 1.03 & 0.91\\
	Dat & 1.27 & 1.01 & 0.00 & 0.87 & 1.47 & 0.76\\
	Gen & 0.82 & 0.42 & 0.87 & 0.00 & 0.84 & 0.87\\
	Loc & 1.09 & 1.03 & 1.47 & 0.84 & 0.00 & 1.48\\
	Nom & 0.94 & 0.91 & 0.76 & 0.87 & 1.48 & 0.00\\
	\bottomrule
\end{tabular}
\caption{Distances de Wasserstein entre les Diagrammes de Persistence des Complexes de Rips pour quelques Cas}
\label{tab_wass_rc}
\end{table}

Cela signifie notamment que la représentation d'un cas comme variété topologique ne varie que peu d'un cas à l'autre, sans toutefois pouvoir tirer plus d'informations que cela.
Intuitivement, cela signifie que si deux cas peuvent avoir des directions \textit{générales} différentes, leurs représentations seront semblables, à une rotation près.

Cependant, d'un point de vue linguistique, il est difficile de tirer des conclusions de ces données. En effet, il faudrait pouvoir comparer les diagrammes pour plus de catégories morphosyntaxiques que simplement quelques cas. Ceci n'étant pas le coeur de notre étude et demandant un temps de calcul assez long, nous n'irons pas plus loin dans ce rapport.

\subsubsection{t-SNE}\label{subsub:tsne}
Dans la suite, nous allons essayer différentes techniques de clustering, afin de vérifier si deux cas supposément distincts peuvent être aisément différenciés.
On essaie d'abord d'appliquer une analyse t-SNE en 2D, décrite dans~\cite{tSNE}.
L'algorithme définit une distribution\footnote{On le note plutôt comme des probabilités conditionnelles, mais c'est équivalent} $D_{1}$ de probabilités sur les paires de points de l'espace de départ (si $x, y$ sont proches alors $D_{1}(x \mid y)$ est proche de $1$) et construit alors une distribution $D_{2}$ sur les paires de points d'un espace à $2$ dimensions minimisant la divergence de Kullback-Leibler $\sum_{x, y}D_{1}(x \mid y)\log\frac{D_{1}(x\mid y)}{D_{2}(x\mid y)}$ entre $D_{1}$ et $D_{2}$.
Ici, on l'applique sur le Génitif et le Nominatif, voir \ref{fig_tsne}
\begin{figure}
\begin{minipage}{.5\textwidth}
	\begin{center}
	\includegraphics[width=\linewidth]{Figures/Visualisations/tsne_Gen_Nom.pdf}
	\end{center}
\end{minipage}
\begin{minipage}{.5\textwidth}
	\begin{center}
	\includegraphics[width=\linewidth]{Figures/Visualisations/tsne_Gen_Nom_Nouns.pdf}
	\end{center}
\end{minipage}
\caption{Représentations de l'Analyse t-SNE à deux Composantes sur le Génitif et le Nominatif}
\label{fig_tsne}
\end{figure}

Il semble que deux clusters se dégagent, avec des frontières toutefois assez floues.
Il semblerait donc que syntaxiquement, le génitif et le nominatif aient des représentations assez différentes, sans toutefois savoir à quel point.

\subsubsection{Clustering avec ToMATo}\label{subsub:tomato}
On applique l'algorithme présenté dans \cite{ToMATo} avec la bibliothèque \texttt{Gudhi} (\cite{Gudhi}) sur des paires de cas, pour essayer de construire des clusters.
Cet algorithme utilise les ensembles de sur-niveau associés à une fonction $f$ (les $F^{\alpha} = f^{-1}\left(\left[\alpha, +\infty\right[\right)$) pour construire le diagramme de persistence (voir \cite{tldrtda} et \cite{ToMATo}).

\begin{figure}
\begin{minipage}{.5\textwidth}
	\includegraphics[width=\linewidth]{Figures/Visualisations/tomato_Nom_Acc_Nouns}
\end{minipage}
\begin{minipage}{.5\textwidth}
	\includegraphics[width=\linewidth]{Figures/Visualisations/tomato_Acc_Gen_Nouns}
\end{minipage}
\caption{Représentations des Clusters trouvés par l'Algorithme ToMATo sur les Paires de $\{\tt Acc,Nom,Gen\}$}
\label{fig_tomato}
\end{figure}

Dans \ref{fig_tomato}, aucun des cas n'est clairement regroupé au sein de certains clusters puisque, comme vu dans \ref{subsub:tda}, les variétés sont très similaires au niveau de la persistence et de l'homologie. L'algorithme ne produit aucun résultat véritablement utilisable d'un point de vue linguistique.

\subsubsection{Clustering avec KNN}\label{subsub:knn}
Pour vérifier l'intuition qui apparaît avec le clustering proposé par l'algorithme t-SNE, on essaie d'appliquer à des listes de cas l'algorithme KNN.
On obtient la matrice de confusion de \ref{tab_knn} en appliquant l'algorithme pour $k = 11$ aux noms qui sont à l'Accusatif, au Génitif, au Locatif ou au Nominatif.
\begin{table}
	\centering
	\pgfplotsset{colormap/jet}
\pgfplotstabletypeset[
	color cells={min=0, max=250},
	col sep=comma,
]{
	Acc,Gen,Loc,Nom
	130,62,51,34
	69,156,16,42
	35,57,29,34
	29,28,9,227
}
	\caption{Heatmap de l'Algorithme KNN avec $k = 11$ sur \texttt{Acc, Gen, Loc, Nom}}
	\label{tab_knn}
\end{table}

On observe notamment que le \emph{Locatif} est plus difficile à reconnaître que les autres cas. Ceci peut venir du faible nombre de langues possédant un locatif en comparaison aux trois autres cas.

\subsection{Conclusion}
Finalement, la comparaison des cas entre eux montre notamment qu'il est impossible d'entièrement différencier tous deux cas.
De plus, du point de vue géométrique, l'ensemble des instances\footnote{des vecteurs représentation} de chaque cas est similaire, indépendamment du cas.
Toutefois, il est clair que plusieurs types de cas se dégagent, et que chacun de ces types est fortement différenciable des autres sémantiquement, mais que cela ne suffit pas à former plusieurs types.

\section{Approche Probabiliste}\label{sec:probas}
On considère désormais qu'à un cas donné on associe une variable aléatoire sur les distributions de relations de dépendances dont on connaît certaines réalisations (les représentations du cas dans les différents corpus).
C'est à dire une variable aléatoire qui a un langage va associer une distribution de probabilité sur les relations de dépendances des mots à ce cas.

\subsection{Barycentrisation}\label{subsec:bary}
On a cherché jusque-là à comparer les cas.
On va désormais essayer de les prototyper\footnote{Trouver un vecteur \textit{typique} du cas} et de mesurer l'écart au prototype, afin de tirer une définition des cas.
Celle-ci devrait coller à la description théorique proposée ci-dessous:
\begin{multicols}{2}
	\begin{description}
		\item[Nom] Agent (sujet) d'un verbe
		\item[Acc] Patient (objet direct) d'un verbe
		\item[Erg] Agent (et donc patient) d'un verbe intransitif (sans objet), ou patient d'un verbe transitif
		\item[Abs] Agent d'un verbe transitif
		\item[Gen] Complément du nom
		\item[Dat] Objet indirect d'un verbe
	\end{description}
\end{multicols}
Pour calculer les prototypes, on procède comme suit:
On calcule l'espérance de la variable aléatoire de chaque cas (en considérant les mesures équiprobables), mais également le barycentre de ses réalisations pour la distance-$1$ de Wasserstein (ou Earth Mover's Distance, voir \cite{PythonPOT} pour plus de détails).
On rappelle par ailleurs que le barycentre de $n$ points pour une distance $d$ est le point $P$ qui minimise l'énergie $E$ associée:

\begin{equation*}
	P = \arg\min_{x} E\left(x, \left(x_{i}\right)_{i \in \onen{n}}\right) \text{ avec } E\left(x, \left(x_{i}\right)_{i \in \onen{n}}\right) = \frac{1}{n}\sum_{i = 1}^{n} d(x, x_{i})
\end{equation*}

Ceci nous donne une forme de vecteur prototypique pour la distribution des relations de dépendance du cas.
On obtient la distribution \ref{tab_proto}, en ne considérant à nouveau que les noms, pour quelques \textit{reldep}:

\begin{table}
\centering
\renewcommand{\arraystretch}{1.3}
\begin{NiceTabular}{llccccc}
				\bf Cas & \bf Prototype &\tt iobj & \tt nmod & \tt nsubj & \tt obj & \tt obl\\
	\multirow[c]{2}{*}{\bf\sc Abs}  & Uniforme    & 0.001 & 0.033 & 0.272 & 0.367 & 0.224\\
									& Wasserstein & 0.000 & 0.016 & 0.286 & 0.522 & 0.112\\

	\multirow[c]{2}{*}{\bf\sc Erg}  & Uniforme    & 0.000 & 0.007 & 0.924 & 0.005 & 0.059\\
									& Wasserstein & 0.000 & 0.005 & 0.976 & 0.014 & 0.003\\

	\multirow[c]{2}{*}{\bf\sc Nom}  & Uniforme    & 0.001 & 0.080 & 0.556 & 0.074 & 0.050\\
									& Wasserstein & 0.000 & 0.049 & 0.654 & 0.093 & 0.038\\

	\multirow[c]{2}{*}{\bf\sc Acc}  & Uniforme    & 0.006 & 0.078 & 0.038 & 0.625 & 0.205\\
									& Wasserstein & 0.000 & 0.072 & 0.019 & 0.576 & 0.259\\

	\multirow[c]{2}{*}{\bf\sc Gen}  & Uniforme    & 0.009 & 0.674 & 0.039 & 0.056 & 0.149\\
									& Wasserstein & 0.000 & 0.729 & 0.031 & 0.045 & 0.179\\

	\multirow[c]{2}{*}{\bf\sc Dat}  & Uniforme    & 0.144 & 0.149 & 0.019 & 0.000 & 0.572\\
									& Wasserstein & 0.190 & 0.164 & 0.005 & 0.000 & 0.605\\

	\multirow[c]{2}{*}{\bf\sc Loc}  & Uniforme    & 0.000 & 0.166 & 0.009 & 0.017 & 0.696\\
									& Wasserstein & 0.000 & 0.188 & 0.000 & 0.000 & 0.762\\

	\multirow[c]{2}{*}{\bf\sc Ins}  & Uniforme    & 0.000 & 0.172 & 0.014 & 0.000 & 0.660\\
									& Wasserstein & 0.000 & 0.213 & 0.000 & 0.000 & 0.738\\

	\multirow[c]{2}{*}{\bf\sc Abl}  & Uniforme    & 0.000 & 0.165 & 0.013 & 0.001 & 0.700\\
									& Wasserstein & 0.000 & 0.172 & 0.000 & 0.000 & 0.785\\
	\CodeAfter
	\begin{tikzpicture}
		\foreach \i in {2, 4,...,19} {\draw[black] (1|-\i) -- (8|-\i);}
		\draw[black] (2|-1) -- (2|-20);
	\end{tikzpicture}
\end{NiceTabular}
\caption{Représentation des Principales \textit{reldep} des Prototypes pour quelques Cas sur les Noms}
\label{tab_proto}
\end{table}

On retrouve bien la représentation attendue, ce qui découle notamment de la définition \emph{théorique} des cas. \\
Il est important de noter que les vecteurs ci-dessus ne permettent pas nécessairement de différencier tous deux cas, mais seulement de différencier leurs usages syntaxiques.
Prenons un exemple:
l'instrumental est le cas qui marque l'outil utilisé pour une action/un objet: Dans une traduction de \textsl{J'ai mangé \emph{avec} une fourchette}, on utiliserait l'instrumental pour \textsl{fourchette}.
Le Locatif est le cas qui marque le lieu d'une action, ou d'un objet: \textsl{J'ai péleriné jusqu'à Saclay} ou \textsl{J'ai couru à côté de l'école} se traduiraient avec des locatifs\footnote{certaines langues comme le finnois possèdent plusieurs \textit{locatifs} appelés inessif, élatif, illatif, adessif, ablatif, allatif\dots\ Ceux-ci servent à différencier différent types de lieux liés à une action: la destination (à Saclay), la direction (vers le Sud), un lieu à proximité (près du Panthéon)\ldots}.
Le locatif et l'instrumental sont principalement des cas morphosémantiques, au sens où ils ne modifient pas la structure de la phrase mais seulement son sens.
Ainsi, il est difficile de les différencier syntaxiquement: ils agissent principalement sur un verbe en précisant son action (\texttt{obl}), mais également sur les noms en précisant leur fonction ou leur position (\texttt{nmod}).
À l'inverse, le nominatif (sujet du verbe), l'accusatif (objet du verbe), le datif (objet indirect du verbe), l'ergatif (sujet du verbe transitif) et l'absolutif (sujet du verbe intransitif et objet du verbe transitif), sont des cas plus fortement marqués syntaxiquement et sont donc plus facilement reconnaissables.

\medskip
Par ailleurs, l'énergie associée au prototype obtenu en prenant la moyenne des réalisations est de l'ordre de celle du barycentre pour la distance-$1$ de Wasserstein.
Comme on pouvait s'y attendre au vu des données, le barycentre pour la distance de Wasserstein présente un profil similaire à la moyenne des instances.

\begin{figure}
\centering
\includegraphics{Figures/Visualisations/Nouns_Wasserstein_Barycenter_Acc}
\caption{Représentation des Données et des Prototypes proposés pour les Noms à l'Accusatif}
\label{fig_proto}
\end{figure}

Sur la figure \ref{fig_proto}, l'axe des abscisses représente les différentes \textit{reldep} apparaissant pour des noms à l'accusatif.
On représente sur le graphe du haut en jaune la fréquence associée à une reldep pour chaque corpus.
Le graphe du bas représente, en rouge, la moyenne uniforme des distributions, en jaune, le barycentre des distributions pour la distance de Wasserstein, et violet la proportion des corpus pour lesquels la \textit{reldep} apparaît.
On vérifie bien notamment que pour la moyenne uniforme, certaines relations sont légèrement représentées car très présentes dans quelques langues, ce qui n'est pas le cas pour le barycentre associé à la distance de Wasserstein.


\subsection{Représentation des adpositions dans \textsc{Universal Dependencies}}\label{subsec:adpos}
Dans UD, certains corpus dénotent, lorsqu'un groupe est combiné avec une adposition (e.g. les prépositions \textsl{à, dans, par} en français, \textsl{to, into, above} en anglais, ou la postposition \textsl{ile} en turc), l'adposition comme gouvernée par la relation \texttt{case} et donnent un cas à l'adposition (cf. l'interjection farsie présentée plus haut \ref{farsi}).
Ceci découle du postulat linguistique selon lequel tous les langages humains sont également expressifs, ce qui implique qu'on peut traduire les cas par des adpositions (en français, tous exceptés le nominatif et l'accusatif).
Dans \cite{morphenglish}, Kirov, Sylak-Flassman, Knowles et Cotterell décrivent une manière d'annoter des corpus en anglais (langue morphologiquement pauvre) selon les caractéristiques morphologiques du tchèque (langue morphologiquement riche). \\
Nous avons donc cherché à déterminer le cas syntaxiquement équivalent à certaines adpositions.
A priori, il n'a aucune raison d'être également le cas sémantiquement équivalent à une adposition, même si l'on considère différents représentants d'une même classe syntaxique de cas (inessif, allatif, essif sont syntaxiquement équivalents parmi les locatifs par exemple).
Pour cela, on compte les relations de dépendances des cibles des relations de dépendance depuis chaque adposition.
On obtient une distribution des usages syntaxiques des adpositions, ce qui permet de comparer une adposition à un marqueur de cas.
On ne donne que la moyenne uniforme des distributions uniformes pour une même langue, le barycentre pour la distance de Wasserstein étant trop volatile pour si peu de distributions (on n'en n'a que 9 pour le français).
\begin{table}
	\centering
	\renewcommand{\arraystretch}{1.3}
	\begin{NiceTabular}{>{\sc}rcccccc}
		\bf Adposition & \tt advcl &\tt iobj & \tt nmod & \tt nsubj & \tt obj & \tt obl\\
		À    & 0.01668 &  & 0.17343 & 0.00048 & 0.00381 & 0.63373\\
		Dans & 0.00466 & & 0.13780 &  & 0.00196 & 0.78694\\
		Par  & 0.00264 &  & 0.13715 & 0.00107 & 0.00178 & 0.74632\\
		Pour & 0.29543 & & 0.15867 &  & 0.00024 & 0.41168\\
		En   & 0.08128 &  & 0.17115 & & 0.00358 & 0.54076\\
		Vers & 0.00262 &  & 0.35741 & & & 0.62160\\
		Avec & 0.00613 & & 0.32369 &  &  & 0.62606\\
		De   & 0.02096 &  & 0.67966 & 0.00138 & 0.01312 & 0.14296\\
		Sans & 0.24451 & & 0.21142 & & 0.00781 & 0.43802\\
		Sous & 0.00217 & & 0.22898 & 0.00020 & 0.72797 & \\
		Sur  & 0.00477 & & 0.36267 &  & 0.00096 & 0.59385\\
		Sauf & 0.10714 & & 0.22619 & & & 0.38095\\
	\CodeAfter
	\begin{tikzpicture}
		\foreach \i in {2,...,13} {\draw[black] (1|-\i) --  (8|-\i);}
	\end{tikzpicture}
	\end{NiceTabular}
	\caption{Représentation de quelques adpositions en français}
	\label{tab:adpos_fr}
\end{table}
Dans la table \ref{tab:adpos_fr} on retrouve le fait que les prépositions françaises s'utilisent dans des constructions similaires, et ne se différencient presque que sémantiquement.
Plus spécifiquement, il semble que \textsl{de} ait un caractère semblable au génitif et que \textsl{à, dans, par, en, vers, avec, sans, sous, sur} ont plutôt un caractère semblable à l'ablatif, au locatif et autres cas syntaxiquement similaires.
Les prépositions \textsl{sauf} et \textsl{pour} quant à elle ont des usages plus particuliers, puisqu'elles servent souvent à introduire des clauses adverbiales (e.g. \textsl{il mange \emph{pour} vivre}).

\section{Conclusion}
Nous avons pu montrer que si les cas ont une structure syntaxique générale assez similaire géométriquement, il existe plusieurs classes syntaxiques assez distinctes au sein desquelles les cas ne diffèrent que sémantiquement.
Il faut également noter que dans le formalisme de \textsc{Universal Dependencies}, avoir nombre d'annotateurs différents cause de fortes disparités dans les manières d'annoter, notamment sur l'annotation de cas sur les adverbes et les adpositions selon la langue.
Ainsi, comme le supposait Martin \textsc{Haspelmath}, il est justifiable d'utiliser des catégories descriptives d'un langage pour en décrire un autre, mais il faut rester vigilant et ne pas calquer d'idiosyncrasies d'une méta-langue de comparaison lors de la description d'une autre.
Pour les cas cela ne pose pas de problème du point de vue syntaxique a priori, mais il est nécessaire de revérifier pour une paire de langues avant de traduire les propriétés syntaxiques d'une langue sur une autre.
\\
Par ailleurs, il n'est pas illogique de donner des cas aux adpositions, de par leurs fonctions sémantiques, mais il apparaît que syntaxiquement elles ne sont que rarement distinguées, et pas toujours de la manière dont on l'attendrait du cas sémantiquement équivalent.

\medskip

Pour complètement finaliser les résultats de cet étude et justifier leur applicabilité, il reste à vérifier:
\begin{itemize}
	\item si un parser syntaxique\footnote{Annotant les mots avec leurs \textit{reldeps}} entraîné sur une langue dont on aurait fusionné des catégories morphosyntaxiques ayant des significations syntaxiques (des vecteurs de \textit{reldep}) proches est indistingable probabilistiquement d'un parser syntaxique entraîné sur cette même langue sans fusions;
	\item si un parser entraîné sur une première langue peut, sur une langue ayant un système de cas proches (à fusion de cas près), obtenir des résultats dignes d'un parser entraîné sur cette seconde langue.
\end{itemize}

Ceci a été étudié par Mathieu \textsc{Dehouck}\footnote{qui se trouve être mon encadrant} durant mon stage.
La vérification n'est pas encore statistiquement robuste, mais les premiers essais indiquent que les deux questions ont des réponses positives.


\appendix

\bibliography{report}
\bibliographystyle{alpha-fr}

\end{document}
